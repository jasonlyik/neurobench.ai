---
title: "Collaborative, Fair, and Representative Benchmarks for Neuromorphic Computing"
layout: splash
permalink: 
date:
header:
  overlay_color: "#777"
  overlay_filter: "0.5"
  overlay_image:
  actions:
    # - label: "To learn more about our approach, check out our whitepaper on arXiv"
    #   url: "https://drive.google.com/file/d/1_4H7X8cSSGfDoHubb7cdAQ48VxhJdOAA/view?usp=sharing"
  caption: ""
excerpt: "NeuroBench is a framework for benchmarking neuromorphic computing algorithms and systems."

sponsors_intro: 
  - title: "Thanks to our lead supporters:"

challenges:
  - image_path: "/assets/images/thumbnails/interface.png"
    alt: "Interface"
    title: "Interface"
    excerpt: "What universal interface is needed for ML Sensors?"

  - image_path: "/assets/images/thumbnails/standards.png"
    alt: "Standards"
    title: "Standards"
    excerpt: "What standards need to be in place for ML Sensors?"

  - image_path: "/assets/images/thumbnails/ethics.png"
    alt: "Ethics"
    title: "Ethics"
    excerpt: "What ethical considerations are needed for ML Sensors?"

---

# NeuroBench v1.0 Preprint

NeuroBench v1.0 includes four defined algorithm benchmarks, algorithmic complexity metric definitions, and algorithm baseline results. System track benchmarks are defined and baselines are currently under development. To learn more, please see our [submission preprint](https://arxiv.org/abs/2304.04640).

# NeuroBench Harness

The NeuroBench harness is an open-source Python package that allows users to easily run the benchmarks and extract useful metrics. Check out the package on [GitHub](https://github.com/NeuroBench/neurobench) and its [documentation](https://neurobench.readthedocs.io/en/latest/).

# Get Involved

To be involved with the NeuroBench community and benchmark project, please join our [mailing list](https://groups.google.com/g/neurobench).